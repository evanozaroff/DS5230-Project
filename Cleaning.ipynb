{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87cK1HK_ceeo"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14669,
     "status": "ok",
     "timestamp": 1649883872460,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "J68KYnLrcY5n",
    "outputId": "fa5a380f-b613-46e2-8b19-9902cd9dbb55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\evano\\miniconda3\\lib\\site-packages (1.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!pip install unidecode\n",
    "from unidecode import unidecode\n",
    "# pip install Counter\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47822,
     "status": "ok",
     "timestamp": 1649884013552,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "7fbF17ydbUvS",
    "outputId": "6323fe5a-2cee-429c-aab6-ef0a34740d0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\evano\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Decompress files and concatenate to datframe\n",
    "filenames = ['UkraineCombinedTweetsDeduped_FEB27.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_MAR01.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_MAR02.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_MAR03.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_MAR04.csv.gzip',\n",
    "             'UkraineCombinedTweetsDeduped_MAR05.csv.gzip']\n",
    "  \n",
    "li = []\n",
    "\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(root + filename, \n",
    "                     index_col=None, \n",
    "                     header=0,\n",
    "                     compression='gzip')\n",
    "    li.append(df)\n",
    "\n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZiD-Ub9Zy"
   },
   "source": [
    "# Cleaning & Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1649884018676,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "MKAWIw-u2EYf"
   },
   "outputs": [],
   "source": [
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1649884019909,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "4yYlKuVDo_Pq",
    "outputId": "afe014b8-782b-4ad4-8b0b-3b5a0bb34627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2984341, 18)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1649884042737,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "i0zQyLvTavgz"
   },
   "outputs": [],
   "source": [
    "# Define function to clean text (to lowercase, remove punctuation, extra whitespace)\n",
    "hashtags = ['#slavaukraini','#russia','#russiaukraineWar','#putin','#russiaukraine', '#russianwar', '#ww3', '#moscow', \n",
    "            '#russianconflict', '#ukraineunderattack', '#ukriane', '#ukraine', '#russianukrainianWar', '#ukrainerussia', \n",
    "            '#ukraineconflict', '#ukrainewar', '#kharkiv', '#stopputinNow']\n",
    "\n",
    "def clean_text(text):\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in '!\"$%&\\'()*+,-./:;<=>?[\\]^_`{|}~']) \n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    \n",
    "    return text_no_doublespace\n",
    "\n",
    "# Define function to determine if a token is junk (@'s, URLs, unwanted hashtags)\n",
    "def is_junk(token):\n",
    "  if ('https' in token) or ('@' in token) or (len(token) <= 2) or (token in hashtags):\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "# Define function to drop unwanted tokens\n",
    "def drop_tokens(tokens):\n",
    "  return list(filter(lambda token: is_junk(token), tokens))\n",
    "\n",
    "# Lemmatize words to get each word to its root\n",
    "def lemmatize_words(words):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lem_words = [lemmatizer.lemmatize(w) for w in words]\n",
    "  return lem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2969,
     "status": "ok",
     "timestamp": 1649884029325,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "LFwO0ij1pbIN",
    "outputId": "40ca73d2-c05a-43b9-f16d-c8ca9be3f8cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1961759, 18)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any tweets not in English\n",
    "data = data.loc[data['language'] == 'en']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 265032,
     "status": "ok",
     "timestamp": 1649884310117,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "KoDhFInO7JFT"
   },
   "outputs": [],
   "source": [
    "# Clean text\n",
    "data.text = data.text.apply(clean_text)\n",
    "\n",
    "# Tokenize text\n",
    "tt = TweetTokenizer()\n",
    "tokenized_text = data.text.apply(tt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 30754,
     "status": "ok",
     "timestamp": 1649884347307,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "_i7mCaNY0L-o"
   },
   "outputs": [],
   "source": [
    "# Drop bad tokens\n",
    "tokenized_text = tokenized_text.apply(drop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "executionInfo": {
     "elapsed": 201639,
     "status": "ok",
     "timestamp": 1649884552496,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "0AYAi3vWeuiw"
   },
   "outputs": [],
   "source": [
    "# Apply lemmetization\n",
    "tokenized_text = tokenized_text.apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1649884556692,
     "user": {
      "displayName": "Maya Webb",
      "userId": "09240942597825439314"
     },
     "user_tz": 240
    },
    "id": "heH4zBXWTldD",
    "outputId": "9e9461de-37ac-4934-deb6-087c3c322711",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [#ukrainerussiawar, captured, russian, soldier...\n",
       "1    [like, everybody, else, rooting, for, ukraine,...\n",
       "3    [elected, game, show, host, and, got, clown, e...\n",
       "4    [ukrainian, soldier, wearing, sunflower, their...\n",
       "5    [russian, troop, destroyed, commercial, oil, b...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output file\n",
    "with open(root + 'cleaned_tokenized_data.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    for row in tokenized_text:\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zgTq919ub2tp"
   ],
   "name": "Project Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
